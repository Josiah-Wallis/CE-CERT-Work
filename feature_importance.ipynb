{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from collections import defaultdict\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# load experimental data\n",
    "data = pd.read_excel('data/data.xlsx', sheet_name = '1st Trial', names = ['time (min)', 'm_xylene', 'NO', 'NO2', 'NOx', 'NOx/m_xy', 'beta', 'NO,NO2 crossing time (min)', 'Wall loss factor', 'Peak O3 Concentration', '50% of Final M0 Time', 'PeakDp', 'deltaHC', 'm_xy consume ratio', 'deltaHC / beta', 'deltaHC * beta', 'M0', 'yield'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-2]\n",
    "y = data.iloc[:, -2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImportanceValidation(model_type, X, y, runs, best, alpha = 0.1):\n",
    "    feature_count = defaultdict(int)\n",
    "    feature_freq = defaultdict(lambda: [0] * best)\n",
    "    feature_avgs = None\n",
    "\n",
    "    if model_type == 'forest':\n",
    "        for _ in range(runs):\n",
    "            state_1 = np.random.randint(1, 1000)\n",
    "            state_2 = np.random.randint(1, 1000)\n",
    "            X_train, _, y_train, _ = train_test_split(X, y, test_size = 0.2, random_state = state_1)\n",
    "\n",
    "            model = RandomForestRegressor(n_estimators = 100, random_state = state_2)\n",
    "            model.fit(X_train, y_train)\n",
    "            importances = model.feature_importances_\n",
    "            feature_importances = pd.Series(importances, index = X.columns)\n",
    "            feature_importances = feature_importances.sort_values(ascending = False)\n",
    "            for i, feature in enumerate(feature_importances.index[:best]):\n",
    "                feature_count[feature] += 1\n",
    "                feature_freq[feature][i] += 1\n",
    "\n",
    "            if feature_avgs is None:\n",
    "                feature_avgs = feature_importances\n",
    "            else:\n",
    "                feature_avgs += feature_importances\n",
    "    else:\n",
    "        feature_counts = []\n",
    "        feature_freqs = []\n",
    "        feature_avgs_list = []\n",
    "        for output in range(y.shape[1]):\n",
    "            for _ in range(runs):\n",
    "                state_1 = np.random.randint(1, 1000)\n",
    "                state_2 = np.random.randint(1, 1000)\n",
    "                X_train, _, y_train, _ = train_test_split(X, y, test_size = 0.2, random_state = state_1)\n",
    "\n",
    "                if model_type == 'lasso':\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train = scaler.fit_transform(X_train)\n",
    "                    model = Lasso(alpha = alpha, max_iter = 5000, random_state = state_2)\n",
    "                    model.fit(X_train, y_train.iloc[:, output])\n",
    "                    coefficients = model.coef_\n",
    "                    feature_coefficients = pd.Series(coefficients, index = X.columns)\n",
    "                    feature_coefficients = feature_coefficients.sort_values(key=lambda x: np.abs(x), ascending=False)\n",
    "                    for i, feature in enumerate(feature_coefficients.index[:best]):\n",
    "                        feature_count[feature] += 1\n",
    "                        feature_freq[feature][i] += 1\n",
    "\n",
    "                    if feature_avgs is None:\n",
    "                        feature_avgs = feature_coefficients\n",
    "                    else:\n",
    "                        feature_avgs += feature_coefficients\n",
    "\n",
    "                elif model_type == 'boosting':\n",
    "                    model = GradientBoostingRegressor(n_estimators = 100, random_state = state_2)\n",
    "                    model.fit(X_train, y_train.iloc[:, output])\n",
    "                    importances = model.feature_importances_\n",
    "                    feature_importances = pd.Series(importances, index = X.columns)\n",
    "                    feature_importances = feature_importances.sort_values(ascending = False)\n",
    "                    for i, feature in enumerate(feature_importances.index[:best]):\n",
    "                        feature_count[feature] += 1\n",
    "                        feature_freq[feature][i] += 1\n",
    "\n",
    "                    if feature_avgs is None:\n",
    "                        feature_avgs = feature_importances\n",
    "                    else:\n",
    "                        feature_avgs += feature_importances\n",
    "\n",
    "            feature_counts.append(feature_count)\n",
    "            feature_freqs.append(feature_freq)\n",
    "            feature_count = defaultdict(int)\n",
    "            feature_freq = defaultdict(lambda: [0] * best)\n",
    "            feature_avgs_list.append((feature_avgs / runs).sort_values(key=lambda x: np.abs(x), ascending = False))\n",
    "            feature_avgs = None\n",
    "\n",
    "    \n",
    "    if model_type == 'forest':\n",
    "        sorted_pairs = sorted(zip(feature_count.values(), list(feature_count)))\n",
    "        best_features =[x[1] for x in reversed(sorted_pairs)]\n",
    "        return feature_count, feature_freq, best_features[:best], (feature_avgs / runs).sort_values(key=lambda x: np.abs(x), ascending = False)\n",
    "    else:\n",
    "        best_set = []\n",
    "        for feature_count in feature_counts:\n",
    "            sorted_pairs = sorted(zip(feature_count.values(), list(feature_count)))\n",
    "            best_features =[x[1] for x in reversed(sorted_pairs)]\n",
    "            best_set.append(best_features[:best])\n",
    "        return feature_counts, feature_freqs, best_set, feature_avgs_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nonlinear\n",
    "feature_count_f, feature_freq_f, best_features_f, feature_avgs_list_f = ImportanceValidation('forest', X, y, 1000, 5)\n",
    "best_features_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "sns.set(style = 'whitegrid')\n",
    "sns.barplot(x = feature_avgs_list_f.values, y = feature_avgs_list_f.index, palette = 'viridis')\n",
    "plt.title('Feature Importance (nonlinear - forest)', fontsize = 16, fontweight = 'bold')\n",
    "plt.xlabel('Importance', fontsize = 14)\n",
    "plt.ylabel('Feature', fontsize = 14)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "\n",
    "sns.despine(left = True, bottom = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear\n",
    "# if lasso doesn't have good coeffs / same frequency items, that means no strong linear dependence between inputs and outputs\n",
    "# show that these results (best_features[0]) show in the correlations because we see high correlations\n",
    "feature_counts_l, feature_freqs_l, best_features_l, feature_avgs_list_l = ImportanceValidation('lasso', X, y, 1000, 5)\n",
    "best_features_l[0], best_features_l[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "sns.set(style = 'whitegrid')\n",
    "sns.barplot(x = np.abs(feature_avgs_list_l[0].values), y = feature_avgs_list_l[0].index, palette = 'viridis')\n",
    "plt.title('Feature Importance w.r.t. M0 (linear - lasso)', fontsize = 16, fontweight = 'bold')\n",
    "plt.xlabel('Importance', fontsize = 14)\n",
    "plt.ylabel('Feature', fontsize = 14)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "\n",
    "sns.despine(left = True, bottom = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "sns.set(style = 'whitegrid')\n",
    "sns.barplot(x = np.abs(feature_avgs_list_l[1].values), y = feature_avgs_list_l[1].index, palette = 'viridis')\n",
    "plt.title('Feature Importance w.r.t. yield (linear - lasso)', fontsize = 16, fontweight = 'bold')\n",
    "plt.xlabel('Importance', fontsize = 14)\n",
    "plt.ylabel('Feature', fontsize = 14)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "\n",
    "sns.despine(left = True, bottom = True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to self: y.iloc[column, column] instead of y[y.columns[slice]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nonlinear\n",
    "feature_counts_b, feature_freqs_b, best_features_b, feature_avgs_list_b = ImportanceValidation('boosting', X, y, 1000, 5)\n",
    "best_features_b[0], best_features_b[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "sns.set(style = 'whitegrid')\n",
    "sns.barplot(x = np.abs(feature_avgs_list_b[0].values), y = feature_avgs_list_b[0].index, palette = 'viridis')\n",
    "plt.title('Feature Importance w.r.t. M0 (nonlinear - boosting)', fontsize = 16, fontweight = 'bold')\n",
    "plt.xlabel('Importance', fontsize = 14)\n",
    "plt.ylabel('Feature', fontsize = 14)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "\n",
    "sns.despine(left = True, bottom = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "sns.set(style = 'whitegrid')\n",
    "sns.barplot(x = np.abs(feature_avgs_list_b[1].values), y = feature_avgs_list_b[1].index, palette = 'viridis')\n",
    "plt.title('Feature Importance w.r.t. yield (nonlinear - boosting)', fontsize = 16, fontweight = 'bold')\n",
    "plt.xlabel('Importance', fontsize = 14)\n",
    "plt.ylabel('Feature', fontsize = 14)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "\n",
    "sns.despine(left = True, bottom = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
